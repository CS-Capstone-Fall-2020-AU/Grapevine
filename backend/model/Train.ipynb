{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Drew\n[nltk_data]     Meseck\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\Drew Meseck\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = get_stop_words('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/spam_review_data.csv', engine= 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                               text polarity  \\\n",
       "0           0  After recent week stay at the Affinia Hotels, ...      pos   \n",
       "1           1  Although much too overpriced in my opinion, th...      pos   \n",
       "2           2  The Affinia hotel in Chicago was superb. the r...      pos   \n",
       "3           3  THIS HOTEL IS FANTASTIC. I stayed there on my ...      pos   \n",
       "4           4  The Affinia Chicago is a wonderful place to st...      pos   \n",
       "\n",
       "  truthful  \n",
       "0      dec  \n",
       "1      dec  \n",
       "2      dec  \n",
       "3      dec  \n",
       "4      dec  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>polarity</th>\n      <th>truthful</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>After recent week stay at the Affinia Hotels, ...</td>\n      <td>pos</td>\n      <td>dec</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Although much too overpriced in my opinion, th...</td>\n      <td>pos</td>\n      <td>dec</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>The Affinia hotel in Chicago was superb. the r...</td>\n      <td>pos</td>\n      <td>dec</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>THIS HOTEL IS FANTASTIC. I stayed there on my ...</td>\n      <td>pos</td>\n      <td>dec</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>The Affinia Chicago is a wonderful place to st...</td>\n      <td>pos</td>\n      <td>dec</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    text = re.sub('[!@#$%^&*-+=_]', '', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [word for word in text if len(word) > 2]\n",
    "    #pbar.update(1)\n",
    "    return nltk.pos_tag(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "sample_text = df.text[0]\n",
    "type(tokenize_data(sample_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknz(col):\n",
    "    return [tokenize_data(row) for row in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = tknz(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                               text polarity  \\\n",
       "0           0  After recent week stay at the Affinia Hotels, ...      pos   \n",
       "1           1  Although much too overpriced in my opinion, th...      pos   \n",
       "2           2  The Affinia hotel in Chicago was superb. the r...      pos   \n",
       "3           3  THIS HOTEL IS FANTASTIC. I stayed there on my ...      pos   \n",
       "4           4  The Affinia Chicago is a wonderful place to st...      pos   \n",
       "\n",
       "  truthful                                             tokens  \n",
       "0      dec  [(After, IN), (recent, JJ), (week, NN), (stay,...  \n",
       "1      dec  [(Although, IN), (much, RB), (overpriced, VBN)...  \n",
       "2      dec  [(The, DT), (Affinia, NNP), (hotel, NN), (Chic...  \n",
       "3      dec  [(THIS, NNP), (HOTEL, NNP), (FANTASTIC, NNP), ...  \n",
       "4      dec  [(The, DT), (Affinia, NNP), (Chicago, NNP), (w...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>polarity</th>\n      <th>truthful</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>After recent week stay at the Affinia Hotels, ...</td>\n      <td>pos</td>\n      <td>dec</td>\n      <td>[(After, IN), (recent, JJ), (week, NN), (stay,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Although much too overpriced in my opinion, th...</td>\n      <td>pos</td>\n      <td>dec</td>\n      <td>[(Although, IN), (much, RB), (overpriced, VBN)...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>The Affinia hotel in Chicago was superb. the r...</td>\n      <td>pos</td>\n      <td>dec</td>\n      <td>[(The, DT), (Affinia, NNP), (hotel, NN), (Chic...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>THIS HOTEL IS FANTASTIC. I stayed there on my ...</td>\n      <td>pos</td>\n      <td>dec</td>\n      <td>[(THIS, NNP), (HOTEL, NNP), (FANTASTIC, NNP), ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>The Affinia Chicago is a wonderful place to st...</td>\n      <td>pos</td>\n      <td>dec</td>\n      <td>[(The, DT), (Affinia, NNP), (Chicago, NNP), (w...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(token, token_index, sent, pos_lab):\n",
    "    #returns different information about the token for characterization\n",
    "    token_feature = {\n",
    "        'token'             : token.lower(),                        #returns the lowercase text of the token itself\n",
    "        'pos_lab'           : pos_lab,                               #Part of speech of the token\n",
    "        'is_first'          : token_index == 0,                     #Is the token the first in the sentence\n",
    "        'is_last'           : token_index == len(sent) - 1,         #Is the token the last in the sentence\n",
    "        \n",
    "        'is_capitalized'    : token[0].upper() == token[0],         #Is the first letter capitalized\n",
    "        'is_all_caps'       : token.upper() == token,               #Is the whole word in caps\n",
    "        'is_numeric'        : token.isdigit(),                      #are there any digits in the token?\n",
    "        \n",
    "        'prefix-1'          : token[0].lower(),                             #token prefix w/one letter\n",
    "        'prefix-2'          : token[:1].lower(),                            #token prefix w/two letters\n",
    "        'suffix-1'          : token[-1].lower(),                            #token suffix w/one letter\n",
    "        'suffix-2'          : '' if len(token) < 2 else token[-2:], #token suffix w/two letters\n",
    "\n",
    "        'prev-token'        : '' if token_index == 0 else sent[token_index - 1].lower(), #token immediately proceeding this token\n",
    "        '2-prev-token'      : '' if token_index >= len(sent) - 2 else sent[token_index - 2][0].lower(), #token two preceeding this token\n",
    "        'next-token'        : '' if token_index == len(sent) - 1 else sent[token_index + 1][0].lower(), #token after this token\n",
    "        '2-next-token'      : '' if token_index >= len(sent) - 2 else sent[token_index + 2][0].lower(), #token 2 after this token\n",
    "    }   \n",
    "    return token_feature\n",
    "    \n",
    "def form_data(reviews):\n",
    "    final_feat = []\n",
    "    for rev in reviews:\n",
    "        features = []\n",
    "        for token_index, token_pair in enumerate(rev):\n",
    "            sentence = [i[0] for i in rev]\n",
    "            token, pos_lab = token_pair\n",
    "            features.append(get_feature(token, token_index, sentence, pos_lab))\n",
    "        final_feat.append(features)\n",
    "    return final_feat\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = form_data(df.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'token': 'although',\n",
       "  'pos_lab': 'IN',\n",
       "  'is_first': True,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': True,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'a',\n",
       "  'prefix-2': 'a',\n",
       "  'suffix-1': 'h',\n",
       "  'suffix-2': 'gh',\n",
       "  'prev-token': '',\n",
       "  '2-prev-token': 'b',\n",
       "  'next-token': 'm',\n",
       "  '2-next-token': 'o'},\n",
       " {'token': 'much',\n",
       "  'pos_lab': 'RB',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'm',\n",
       "  'prefix-2': 'm',\n",
       "  'suffix-1': 'h',\n",
       "  'suffix-2': 'ch',\n",
       "  'prev-token': 'although',\n",
       "  '2-prev-token': 'a',\n",
       "  'next-token': 'o',\n",
       "  '2-next-token': 'o'},\n",
       " {'token': 'overpriced',\n",
       "  'pos_lab': 'VBN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'o',\n",
       "  'prefix-2': 'o',\n",
       "  'suffix-1': 'd',\n",
       "  'suffix-2': 'ed',\n",
       "  'prev-token': 'much',\n",
       "  '2-prev-token': 'a',\n",
       "  'next-token': 'o',\n",
       "  '2-next-token': 'h'},\n",
       " {'token': 'opinion',\n",
       "  'pos_lab': 'NN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'o',\n",
       "  'prefix-2': 'o',\n",
       "  'suffix-1': 'n',\n",
       "  'suffix-2': 'on',\n",
       "  'prev-token': 'overpriced',\n",
       "  '2-prev-token': 'm',\n",
       "  'next-token': 'h',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'hotel',\n",
       "  'pos_lab': 'NN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'h',\n",
       "  'prefix-2': 'h',\n",
       "  'suffix-1': 'l',\n",
       "  'suffix-2': 'el',\n",
       "  'prev-token': 'opinion',\n",
       "  '2-prev-token': 'o',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 't'},\n",
       " {'token': 'spotless',\n",
       "  'pos_lab': 'VBD',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 's',\n",
       "  'suffix-2': 'ss',\n",
       "  'prev-token': 'hotel',\n",
       "  '2-prev-token': 'o',\n",
       "  'next-token': 't',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'the',\n",
       "  'pos_lab': 'DT',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': True,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 't',\n",
       "  'prefix-2': 't',\n",
       "  'suffix-1': 'e',\n",
       "  'suffix-2': 'he',\n",
       "  'prev-token': 'spotless',\n",
       "  '2-prev-token': 'h',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 'c'},\n",
       " {'token': 'staff',\n",
       "  'pos_lab': 'NN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 'f',\n",
       "  'suffix-2': 'ff',\n",
       "  'prev-token': 'the',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 'c',\n",
       "  '2-next-token': 'a'},\n",
       " {'token': 'courteous',\n",
       "  'pos_lab': 'JJ',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'c',\n",
       "  'prefix-2': 'c',\n",
       "  'suffix-1': 's',\n",
       "  'suffix-2': 'us',\n",
       "  'prev-token': 'staff',\n",
       "  '2-prev-token': 't',\n",
       "  'next-token': 'a',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'and',\n",
       "  'pos_lab': 'CC',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': True,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'a',\n",
       "  'prefix-2': 'a',\n",
       "  'suffix-1': 'd',\n",
       "  'suffix-2': 'nd',\n",
       "  'prev-token': 'courteous',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'spa',\n",
       "  'pos_lab': 'JJ',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 'a',\n",
       "  'suffix-2': 'pa',\n",
       "  'prev-token': 'and',\n",
       "  '2-prev-token': 'c',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 'w'},\n",
       " {'token': 'service',\n",
       "  'pos_lab': 'NN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 'e',\n",
       "  'suffix-2': 'ce',\n",
       "  'prev-token': 'spa',\n",
       "  '2-prev-token': 'a',\n",
       "  'next-token': 'w',\n",
       "  '2-next-token': 'g'},\n",
       " {'token': 'was',\n",
       "  'pos_lab': 'NNP',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': True,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'w',\n",
       "  'prefix-2': 'w',\n",
       "  'suffix-1': 's',\n",
       "  'suffix-2': 'as',\n",
       "  'prev-token': 'service',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 'g',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'god',\n",
       "  'pos_lab': 'NNP',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': True,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'g',\n",
       "  'prefix-2': 'g',\n",
       "  'suffix-1': 'd',\n",
       "  'suffix-2': 'od',\n",
       "  'prev-token': 'was',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 'r'},\n",
       " {'token': 'send',\n",
       "  'pos_lab': 'VBP',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 'd',\n",
       "  'suffix-2': 'nd',\n",
       "  'prev-token': 'god',\n",
       "  '2-prev-token': 'w',\n",
       "  'next-token': 'r',\n",
       "  '2-next-token': 'f'},\n",
       " {'token': 'relatively',\n",
       "  'pos_lab': 'RB',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'r',\n",
       "  'prefix-2': 'r',\n",
       "  'suffix-1': 'y',\n",
       "  'suffix-2': 'ly',\n",
       "  'prev-token': 'send',\n",
       "  '2-prev-token': 'g',\n",
       "  'next-token': 'f',\n",
       "  '2-next-token': 'l'},\n",
       " {'token': 'flexible',\n",
       "  'pos_lab': 'JJ',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'f',\n",
       "  'prefix-2': 'f',\n",
       "  'suffix-1': 'e',\n",
       "  'suffix-2': 'le',\n",
       "  'prev-token': 'relatively',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 'l',\n",
       "  '2-next-token': 't'},\n",
       " {'token': 'location',\n",
       "  'pos_lab': 'NN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'l',\n",
       "  'prefix-2': 'l',\n",
       "  'suffix-1': 'n',\n",
       "  'suffix-2': 'on',\n",
       "  'prev-token': 'flexible',\n",
       "  '2-prev-token': 'r',\n",
       "  'next-token': 't',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'traveling',\n",
       "  'pos_lab': 'VBG',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 't',\n",
       "  'prefix-2': 't',\n",
       "  'suffix-1': 'g',\n",
       "  'suffix-2': 'ng',\n",
       "  'prev-token': 'location',\n",
       "  '2-prev-token': 'f',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'sight',\n",
       "  'pos_lab': 'VBD',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 't',\n",
       "  'suffix-2': 'ht',\n",
       "  'prev-token': 'traveling',\n",
       "  '2-prev-token': 'l',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 'd'},\n",
       " {'token': 'seeing',\n",
       "  'pos_lab': 'VBG',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 'g',\n",
       "  'suffix-2': 'ng',\n",
       "  'prev-token': 'sight',\n",
       "  '2-prev-token': 't',\n",
       "  'next-token': 'd',\n",
       "  '2-next-token': 's'},\n",
       " {'token': 'didnt',\n",
       "  'pos_lab': 'JJ',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'd',\n",
       "  'prefix-2': 'd',\n",
       "  'suffix-1': 't',\n",
       "  'suffix-2': 'nt',\n",
       "  'prev-token': 'seeing',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 's',\n",
       "  '2-next-token': 'm'},\n",
       " {'token': 'spend',\n",
       "  'pos_lab': 'JJ',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 's',\n",
       "  'prefix-2': 's',\n",
       "  'suffix-1': 'd',\n",
       "  'suffix-2': 'nd',\n",
       "  'prev-token': 'didnt',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 'm',\n",
       "  '2-next-token': 'b'},\n",
       " {'token': 'major',\n",
       "  'pos_lab': 'JJ',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'm',\n",
       "  'prefix-2': 'm',\n",
       "  'suffix-1': 'r',\n",
       "  'suffix-2': 'or',\n",
       "  'prev-token': 'spend',\n",
       "  '2-prev-token': 'd',\n",
       "  'next-token': 'b',\n",
       "  '2-next-token': 't'},\n",
       " {'token': 'bucks',\n",
       "  'pos_lab': 'NNS',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'b',\n",
       "  'prefix-2': 'b',\n",
       "  'suffix-1': 's',\n",
       "  'suffix-2': 'ks',\n",
       "  'prev-token': 'major',\n",
       "  '2-prev-token': 's',\n",
       "  'next-token': 't',\n",
       "  '2-next-token': 'g'},\n",
       " {'token': 'trying',\n",
       "  'pos_lab': 'VBG',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 't',\n",
       "  'prefix-2': 't',\n",
       "  'suffix-1': 'g',\n",
       "  'suffix-2': 'ng',\n",
       "  'prev-token': 'bucks',\n",
       "  '2-prev-token': 'm',\n",
       "  'next-token': 'g',\n",
       "  '2-next-token': 'a'},\n",
       " {'token': 'get',\n",
       "  'pos_lab': 'VB',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'g',\n",
       "  'prefix-2': 'g',\n",
       "  'suffix-1': 't',\n",
       "  'suffix-2': 'et',\n",
       "  'prev-token': 'trying',\n",
       "  '2-prev-token': 'b',\n",
       "  'next-token': 'a',\n",
       "  '2-next-token': 'c'},\n",
       " {'token': 'around',\n",
       "  'pos_lab': 'IN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'a',\n",
       "  'prefix-2': 'a',\n",
       "  'suffix-1': 'd',\n",
       "  'suffix-2': 'nd',\n",
       "  'prev-token': 'get',\n",
       "  '2-prev-token': 't',\n",
       "  'next-token': 'c',\n",
       "  '2-next-token': 'l'},\n",
       " {'token': 'city',\n",
       "  'pos_lab': 'NN',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'c',\n",
       "  'prefix-2': 'c',\n",
       "  'suffix-1': 'y',\n",
       "  'suffix-2': 'ty',\n",
       "  'prev-token': 'around',\n",
       "  '2-prev-token': 'g',\n",
       "  'next-token': 'l',\n",
       "  '2-next-token': 'g'},\n",
       " {'token': 'love',\n",
       "  'pos_lab': 'NNP',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': True,\n",
       "  'is_all_caps': True,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'l',\n",
       "  'prefix-2': 'l',\n",
       "  'suffix-1': 'e',\n",
       "  'suffix-2': 'VE',\n",
       "  'prev-token': 'city',\n",
       "  '2-prev-token': 'a',\n",
       "  'next-token': 'g',\n",
       "  '2-next-token': 'b'},\n",
       " {'token': 'going',\n",
       "  'pos_lab': 'NNP',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': True,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'g',\n",
       "  'prefix-2': 'g',\n",
       "  'suffix-1': 'g',\n",
       "  'suffix-2': 'ng',\n",
       "  'prev-token': 'love',\n",
       "  '2-prev-token': 'c',\n",
       "  'next-token': 'b',\n",
       "  '2-next-token': 'a'},\n",
       " {'token': 'back',\n",
       "  'pos_lab': 'RB',\n",
       "  'is_first': False,\n",
       "  'is_last': False,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'b',\n",
       "  'prefix-2': 'b',\n",
       "  'suffix-1': 'k',\n",
       "  'suffix-2': 'ck',\n",
       "  'prev-token': 'going',\n",
       "  '2-prev-token': '',\n",
       "  'next-token': 'a',\n",
       "  '2-next-token': ''},\n",
       " {'token': 'anniversary',\n",
       "  'pos_lab': 'JJ',\n",
       "  'is_first': False,\n",
       "  'is_last': True,\n",
       "  'is_capitalized': False,\n",
       "  'is_all_caps': False,\n",
       "  'is_numeric': False,\n",
       "  'prefix-1': 'a',\n",
       "  'prefix-2': 'a',\n",
       "  'suffix-1': 'y',\n",
       "  'suffix-2': 'ry',\n",
       "  'prev-token': 'back',\n",
       "  '2-prev-token': '',\n",
       "  'next-token': '',\n",
       "  '2-next-token': ''}]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4d4cd283da16e0e4810f47139f06319c6f45ef4d741cdcb652f7d97adbe1ec43"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}